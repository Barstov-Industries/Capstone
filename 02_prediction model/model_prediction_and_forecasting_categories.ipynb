{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling and Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Machine Learning Models and Evaluation Metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Visualization (Optional)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Date Handling\n",
    "from datetime import timedelta\n",
    "\n",
    "# Suppress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 172977 entries, 0 to 172976\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   product_type_no    172977 non-null  int64  \n",
      " 1   product_type_name  172977 non-null  object \n",
      " 2   colour_group_code  172977 non-null  int64  \n",
      " 3   colour_group_name  172977 non-null  object \n",
      " 4   week               172977 non-null  object \n",
      " 5   average_price      172977 non-null  float64\n",
      " 6   total_units_sold   172977 non-null  int64  \n",
      " 7   unique_customers   172977 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(3)\n",
      "memory usage: 10.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "config = dotenv_values()\n",
    "\n",
    "# Define variables for the login\n",
    "pg_user = config['POSTGRES_USER']\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "pg_schema = config['POSTGRES_SCHEMA']\n",
    "pg_pass = config['POSTGRES_PASS']\n",
    "\n",
    "# Set up the PostgreSQL connection URL\n",
    "url = f'postgresql://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}'\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine(url, echo=False)\n",
    "my_schema = 'capstone_barstov_industries'\n",
    "\n",
    "# Load data directly into a DataFrame\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(f'SET search_path TO {my_schema};'))\n",
    "    data = pd.read_sql(\"SELECT * FROM model_data_week;\", conn)\n",
    "\n",
    "# Check the DataFrame structure\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure 'week' is in datetime format without timezone\n",
    "data['week'] = pd.to_datetime(data['week']).dt.tz_localize(None)\n",
    "\n",
    "# Filter data up to March 2020 and sort\n",
    "data = data[data['week'] <= \"2020-03-01\"].copy()\n",
    "data = data.sort_values(by=['product_type_no', 'colour_group_code', 'week'])\n",
    "\n",
    "# Step 1: Data-Driven Peak Season Identification Using Quantile-Based Thresholds\n",
    "demand_clusters = data.groupby(['product_type_no', 'colour_group_code', 'week'])['total_units_sold'].sum().reset_index()\n",
    "demand_clusters['peak_threshold'] = demand_clusters.groupby(['product_type_no', 'colour_group_code'])['total_units_sold'].transform(lambda x: x.quantile(0.90))\n",
    "demand_clusters['is_peak_season'] = demand_clusters['total_units_sold'] >= demand_clusters['peak_threshold']\n",
    "\n",
    "# Merge peak season flags back into main dataset\n",
    "data = pd.merge(data, demand_clusters[['product_type_no', 'colour_group_code', 'week', 'is_peak_season']], on=['product_type_no', 'colour_group_code', 'week'], how='left')\n",
    "\n",
    "# Remove duplicate columns if any\n",
    "if 'is_peak_season_x' in data.columns and 'is_peak_season_y' in data.columns:\n",
    "    data = data.drop(columns=['is_peak_season_x']).rename(columns={'is_peak_season_y': 'is_peak_season'})\n",
    "\n",
    "# Step 2: Feature Engineering - Lagged Features and Seasonal Features\n",
    "data['lag_units_sold_1week'] = data.groupby(['product_type_no', 'colour_group_code'])['total_units_sold'].shift(1)\n",
    "data['lag_units_sold_2weeks'] = data.groupby(['product_type_no', 'colour_group_code'])['total_units_sold'].shift(2)\n",
    "data = data.dropna(subset=['lag_units_sold_1week', 'lag_units_sold_2weeks']).reset_index(drop=True)\n",
    "\n",
    "# Month feature for seasonality\n",
    "data['month'] = data['week'].dt.month\n",
    "\n",
    "# Scale the price\n",
    "data['average_price'] = data['average_price'] * 10\n",
    "\n",
    "# Reapply the train, validation, and test splits\n",
    "train_data = data[(data['week'] >= \"2019-01-01\") & (data['week'] < \"2020-01-01\")]\n",
    "validation_data = data[(data['week'] >= \"2020-01-01\") & (data['week'] < \"2020-02-01\")]\n",
    "test_data = data[(data['week'] >= \"2020-02-01\") & (data['week'] < \"2020-03-01\")]\n",
    "\n",
    "# Print sizes for verification\n",
    "print(\"Training set size:\", train_data.shape)\n",
    "print(\"Validation set size:\", validation_data.shape)\n",
    "print(\"Test set size:\", test_data.shape)\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = ['product_type_no', 'colour_group_code', 'average_price',\n",
    "                   'lag_units_sold_1week', 'lag_units_sold_2weeks', 'month', 'is_peak_season']\n",
    "\n",
    "X_train, y_train = train_data[feature_columns], train_data['total_units_sold']\n",
    "X_validation, y_validation = validation_data[feature_columns], validation_data['total_units_sold']\n",
    "X_test, y_test = test_data[feature_columns], test_data['total_units_sold']\n",
    "\n",
    "# Initialize and train base models\n",
    "model_rf = RandomForestRegressor(n_estimators=200, max_depth=30, random_state=42)\n",
    "model_lr = LinearRegression()\n",
    "model_mlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "\n",
    "model_rf.fit(X_train, y_train)\n",
    "model_lr.fit(X_train, y_train)\n",
    "model_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions and round to integers\n",
    "preds_rf_val = np.round(model_rf.predict(X_validation)).astype(int)\n",
    "preds_lr_val = np.round(model_lr.predict(X_validation)).astype(int)\n",
    "preds_mlp_val = np.round(model_mlp.predict(X_validation)).astype(int)\n",
    "\n",
    "preds_rf_test = np.round(model_rf.predict(X_test)).astype(int)\n",
    "preds_lr_test = np.round(model_lr.predict(X_test)).astype(int)\n",
    "preds_mlp_test = np.round(model_mlp.predict(X_test)).astype(int)\n",
    "\n",
    "# Meta-model predictions\n",
    "val_predictions_df = pd.DataFrame({'RandomForest': preds_rf_val, 'LinearRegression': preds_lr_val, 'MLPRegressor': preds_mlp_val})\n",
    "test_predictions_df = pd.DataFrame({'RandomForest': preds_rf_test, 'LinearRegression': preds_lr_test, 'MLPRegressor': preds_mlp_test})\n",
    "\n",
    "final_model_rf = RandomForestRegressor(n_estimators=200, max_depth=30, random_state=42)\n",
    "final_model_rf.fit(val_predictions_df, y_validation)\n",
    "\n",
    "# Generate and round meta-model predictions\n",
    "final_preds_val = np.round(final_model_rf.predict(val_predictions_df)).astype(int)\n",
    "final_preds_test = np.round(final_model_rf.predict(test_predictions_df)).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_val = mean_absolute_error(y_validation, final_preds_val)\n",
    "rmse_val = mean_squared_error(y_validation, final_preds_val, squared=False)\n",
    "mae_test = mean_absolute_error(y_test, final_preds_test)\n",
    "rmse_test = mean_squared_error(y_test, final_preds_test, squared=False)\n",
    "\n",
    "print(\"Extended Validation MAE:\", mae_val)\n",
    "print(\"Extended Validation RMSE:\", rmse_val)\n",
    "print(\"Test MAE:\", mae_test)\n",
    "print(\"Test RMSE:\", rmse_test)\n",
    "\n",
    "# Aggregate inventory needs by product type and color for the test period\n",
    "test_data['predicted_inventory_needs'] = final_preds_test\n",
    "inventory_needs = test_data.groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'].sum().reset_index()\n",
    "\n",
    "# Calculate average demand for each product-type and color combination\n",
    "avg_demand = data.groupby(['product_type_no', 'colour_group_code'])['total_units_sold'].mean().reset_index()\n",
    "avg_demand = avg_demand.rename(columns={'total_units_sold': 'avg_demand'})\n",
    "\n",
    "# Merge average demand back into `test_data`\n",
    "test_data = pd.merge(test_data, avg_demand, on=['product_type_no', 'colour_group_code'], how='left')\n",
    "\n",
    "# Define multipliers for peak and non-peak seasons\n",
    "non_peak_season_multiplier = 0.5  # Trigger rebalancing sooner for non-peak items (50% of average demand)\n",
    "peak_season_multiplier = 1.0      # Allow peak items more space (100% of average demand)\n",
    "\n",
    "# Calculate the low-demand threshold dynamically based on the seasonality of each item\n",
    "test_data['low_demand_threshold'] = np.where(\n",
    "    test_data['is_peak_season'],\n",
    "    test_data['avg_demand'] * peak_season_multiplier,     # Use peak multiplier for peak season items\n",
    "    test_data['avg_demand'] * non_peak_season_multiplier  # Use non-peak multiplier for non-peak items\n",
    ")\n",
    "\n",
    "# Calculate a rolling 4-week sum of predicted inventory needs to identify low-demand periods\n",
    "test_data['low_demand_4weeks'] = test_data.groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'] \\\n",
    "    .transform(lambda x: x.rolling(window=4, min_periods=1).sum())\n",
    "\n",
    "# Flag items for rebalancing based on the dynamic low-demand threshold\n",
    "test_data['rebalance_flag'] = test_data['low_demand_4weeks'] <= test_data['low_demand_threshold']\n",
    "\n",
    "# Summarize the flagged items for rebalancing\n",
    "rebalance_needs = test_data[test_data['rebalance_flag']].groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'].sum().reset_index()\n",
    "rebalance_needs['rebalance_units'] = np.round(rebalance_needs['predicted_inventory_needs']).astype(int)\n",
    "\n",
    "# Summarize the overall predicted inventory needs\n",
    "inventory_needs = test_data.groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'].sum().reset_index()\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPredicted Inventory Needs (Test Period):\")\n",
    "print(inventory_needs)\n",
    "\n",
    "print(\"\\nPotential Rebalance Needs (Dynamic Low Demand Thresholds):\")\n",
    "print(rebalance_needs)\n",
    "\n",
    "print(\"Total Predicted Inventory Needs (Test Period):\", int(inventory_needs['predicted_inventory_needs'].sum()))\n",
    "print(\"Total Rebalance Needs (Low Demand Adjustments):\", int(rebalance_needs['rebalance_units'].sum()))\n",
    "print(\"Total Actual Units Sold (Test Period):\", int(y_test.sum()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
