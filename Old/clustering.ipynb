{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from sqlalchemy import create_engine, types\n",
    "from sqlalchemy import text # to be able to pass string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config = dotenv_values()\n",
    "\n",
    "# define variables for the login\n",
    "pg_user = config['POSTGRES_USER']  # align the key label with your .env file !\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "pg_schema = config['POSTGRES_SCHEMA']\n",
    "pg_pass = config['POSTGRES_PASS']\n",
    "\n",
    "\n",
    "url = f'postgresql://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(url, echo=False)\n",
    "my_schema = 'capstone_barstov_industries'\n",
    "\n",
    "with engine.begin() as conn: \n",
    "    result = conn.execute(text(f'SET search_path TO {my_schema};'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as conn: \n",
    "    result = conn.execute(text(f'''\n",
    "                               SELECT * FROM product_analysis; \n",
    "                                '''))\n",
    "    data = result.all()\n",
    "\n",
    "### Let's create a dataframe out of that\n",
    "product_analysis = pd.DataFrame(data) \n",
    "product_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by product type and color group to summarize demand\n",
    "product_data = product_analysis.groupby(['product_type_no', 'colour_group_code', 'garment_group_no']).agg({\n",
    "    'units_sold': 'sum'  # Aggregated demand\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select features for scaling\n",
    "features = product_data[['units_sold', 'product_type_no', 'colour_group_code', 'garment_group_no']]\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "# Initialize HDBSCAN with chosen parameters\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(min_samples=5, min_cluster_size=10)\n",
    "product_clusters = hdbscan_clusterer.fit_predict(scaled_data)\n",
    "\n",
    "# Add cluster labels back to the data\n",
    "product_data['cluster'] = product_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Display cluster distribution\n",
    "unique, counts = np.unique(product_clusters, return_counts=True)\n",
    "cluster_distribution = dict(zip(unique, counts))\n",
    "print(\"Cluster distribution:\", cluster_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize average demand and characteristics by cluster\n",
    "cluster_summary = product_data.groupby('cluster').mean()\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate data by cluster and month for ARIMA\n",
    "cluster_time_data = product_analysis.groupby(['cluster', product_analysis['t_date'].dt.to_period('M')]).agg({\n",
    "    'units_sold': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename columns for clarity\n",
    "cluster_time_data.columns = ['cluster', 'month', 'units_sold']\n",
    "\n",
    "# Convert period to datetime format for ARIMA compatibility\n",
    "cluster_time_data['month'] = cluster_time_data['month'].dt.to_timestamp()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for time series by cluster\n",
    "cluster_series_dict = {cluster: data[['month', 'units_sold']].set_index('month') \n",
    "                       for cluster, data in cluster_time_data.groupby('cluster')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Example parameters for ARIMA (adjust as needed)\n",
    "arima_order = (1, 1, 1)\n",
    "\n",
    "# Dictionary to store ARIMA results for each cluster\n",
    "arima_results = {}\n",
    "\n",
    "# Loop through each cluster and fit ARIMA\n",
    "for cluster, series in cluster_series_dict.items():\n",
    "    # Fill missing months with 0, if any\n",
    "    series = series.asfreq('M', fill_value=0)\n",
    "    \n",
    "    # Fit ARIMA model to each clusterâ€™s time series\n",
    "    try:\n",
    "        model = ARIMA(series['units_sold'], order=arima_order)\n",
    "        arima_fit = model.fit()\n",
    "        \n",
    "        # Save model summary or forecast to results dictionary\n",
    "        arima_results[cluster] = arima_fit.summary()  # Or save arima_fit.forecast(steps=12) for predictions\n",
    "        print(f\"ARIMA model fit for Cluster {cluster}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not fit ARIMA for Cluster {cluster}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
