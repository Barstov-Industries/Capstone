{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling and Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Machine Learning Models and Evaluation Metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Visualization (Optional, if you'd like to visualize predictions or residuals)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 172968 entries, 0 to 172967\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count   Dtype              \n",
      "---  ------             --------------   -----              \n",
      " 0   product_type_no    172968 non-null  int64              \n",
      " 1   product_type_name  172968 non-null  object             \n",
      " 2   colour_group_code  172968 non-null  int64              \n",
      " 3   colour_group_name  172968 non-null  object             \n",
      " 4   week               172968 non-null  datetime64[ns, UTC]\n",
      " 5   average_price      172968 non-null  float64            \n",
      " 6   total_units_sold   172968 non-null  int64              \n",
      " 7   unique_customers   172968 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), int64(4), object(2)\n",
      "memory usage: 10.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "config = dotenv_values()\n",
    "\n",
    "# Define variables for the login\n",
    "pg_user = config['POSTGRES_USER']\n",
    "pg_host = config['POSTGRES_HOST']\n",
    "pg_port = config['POSTGRES_PORT']\n",
    "pg_db = config['POSTGRES_DB']\n",
    "pg_schema = config['POSTGRES_SCHEMA']\n",
    "pg_pass = config['POSTGRES_PASS']\n",
    "\n",
    "# Set up the PostgreSQL connection URL\n",
    "url = f'postgresql://{pg_user}:{pg_pass}@{pg_host}:{pg_port}/{pg_db}'\n",
    "\n",
    "# Create the database engine\n",
    "engine = create_engine(url, echo=False)\n",
    "my_schema = 'capstone_barstov_industries'\n",
    "\n",
    "# Load data directly into a DataFrame\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(f'SET search_path TO {my_schema};'))\n",
    "    data = pd.read_sql(\"SELECT * FROM model_data_week;\", conn)\n",
    "\n",
    "# Check the DataFrame structure\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Ensure 'week' is in datetime format without timezone\n",
    "data['week'] = pd.to_datetime(data['week']).dt.tz_localize(None)\n",
    "\n",
    "# Filter data up to March 2020 and sort\n",
    "data = data[data['week'] <= \"2020-03-01\"].copy()\n",
    "data = data.sort_values(by=['product_type_no', 'colour_group_code', 'week'])\n",
    "\n",
    "# Feature Engineering: Lagged Features and Seasonal Features\n",
    "data['lag_units_sold_1week'] = data.groupby(['product_type_no', 'colour_group_code'])['total_units_sold'].shift(1)\n",
    "data['lag_units_sold_2weeks'] = data.groupby(['product_type_no', 'colour_group_code'])['total_units_sold'].shift(2)\n",
    "data = data.dropna(subset=['lag_units_sold_1week', 'lag_units_sold_2weeks']).reset_index(drop=True)\n",
    "\n",
    "# Month feature for seasonality\n",
    "data['month'] = data['week'].dt.month\n",
    "\n",
    "# Scale the price\n",
    "data['average_price'] = data['average_price'] * 10\n",
    "\n",
    "# Define train, validation, and test sets\n",
    "train_data = data[data['week'] < \"2020-01-01\"]\n",
    "validation_data = data[(data['week'] >= \"2020-01-01\") & (data['week'] < \"2020-03-01\")]\n",
    "test_data = data[data['week'] == data['week'].max()]\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\n",
    "    'product_type_no', 'colour_group_code', 'average_price',\n",
    "    'lag_units_sold_1week', 'lag_units_sold_2weeks', 'month'\n",
    "]\n",
    "\n",
    "X_train, y_train = train_data[feature_columns], train_data['total_units_sold']\n",
    "X_validation, y_validation = validation_data[feature_columns], validation_data['total_units_sold']\n",
    "X_test, y_test = test_data[feature_columns], test_data['total_units_sold']\n",
    "\n",
    "# Initialize and train base models\n",
    "model_rf = RandomForestRegressor(n_estimators=200, max_depth=30, random_state=42)\n",
    "model_lr = LinearRegression()\n",
    "model_mlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "\n",
    "model_rf.fit(X_train, y_train)\n",
    "model_lr.fit(X_train, y_train)\n",
    "model_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions and round to integers\n",
    "preds_rf_val = np.round(model_rf.predict(X_validation)).astype(int)\n",
    "preds_lr_val = np.round(model_lr.predict(X_validation)).astype(int)\n",
    "preds_mlp_val = np.round(model_mlp.predict(X_validation)).astype(int)\n",
    "\n",
    "preds_rf_test = np.round(model_rf.predict(X_test)).astype(int)\n",
    "preds_lr_test = np.round(model_lr.predict(X_test)).astype(int)\n",
    "preds_mlp_test = np.round(model_mlp.predict(X_test)).astype(int)\n",
    "\n",
    "# Meta-model predictions\n",
    "val_predictions_df = pd.DataFrame({'RandomForest': preds_rf_val, 'LinearRegression': preds_lr_val, 'MLPRegressor': preds_mlp_val})\n",
    "test_predictions_df = pd.DataFrame({'RandomForest': preds_rf_test, 'LinearRegression': preds_lr_test, 'MLPRegressor': preds_mlp_test})\n",
    "\n",
    "final_model_rf = RandomForestRegressor(n_estimators=200, max_depth=30, random_state=42)\n",
    "final_model_rf.fit(val_predictions_df, y_validation)\n",
    "\n",
    "# Generate and round meta-model predictions\n",
    "final_preds_val = np.round(final_model_rf.predict(val_predictions_df)).astype(int)\n",
    "final_preds_test = np.round(final_model_rf.predict(test_predictions_df)).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_val = mean_absolute_error(y_validation, final_preds_val)\n",
    "rmse_val = mean_squared_error(y_validation, final_preds_val, squared=False)\n",
    "mae_test = mean_absolute_error(y_test, final_preds_test)\n",
    "rmse_test = mean_squared_error(y_test, final_preds_test, squared=False)\n",
    "\n",
    "print(\"Extended Validation MAE:\", mae_val)\n",
    "print(\"Extended Validation RMSE:\", rmse_val)\n",
    "print(\"Test MAE:\", mae_test)\n",
    "print(\"Test RMSE:\", rmse_test)\n",
    "\n",
    "# Aggregate inventory needs by product type and color for the test period\n",
    "test_data['predicted_inventory_needs'] = final_preds_test\n",
    "inventory_needs = test_data.groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'].sum().reset_index()\n",
    "\n",
    "# Data-Driven Peak Season Identification Using Clustering\n",
    "demand_clusters = data.groupby(['product_type_no', 'colour_group_code', 'week'])['total_units_sold'].sum().reset_index()\n",
    "\n",
    "# Replace .dt.week with .dt.isocalendar().week\n",
    "demand_clusters['week_of_year'] = demand_clusters['week'].dt.isocalendar().week\n",
    "\n",
    "# Fit KMeans to categorize demand into high and low periods\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "demand_clusters['demand_cluster'] = kmeans.fit_predict(demand_clusters[['total_units_sold']])\n",
    "\n",
    "# Identify which cluster is the peak season based on mean demand\n",
    "peak_cluster = demand_clusters.groupby('demand_cluster')['total_units_sold'].mean().idxmax()\n",
    "demand_clusters['is_peak_season'] = demand_clusters['demand_cluster'] == peak_cluster\n",
    "\n",
    "# Merge peak season flags back into main dataset\n",
    "data = pd.merge(data, demand_clusters[['product_type_no', 'colour_group_code', 'week', 'is_peak_season']], on=['product_type_no', 'colour_group_code', 'week'], how='left')\n",
    "\n",
    "\n",
    "# Rebalance Strategy Using Low-Demand Thresholds Instead of Negative Values\n",
    "low_demand_threshold = 5  # Adjust based on data analysis\n",
    "\n",
    "# Calculate a rolling 4-week sum and apply low-demand threshold for rebalancing flag\n",
    "test_data['low_demand_4weeks'] = test_data.groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'] \\\n",
    "    .transform(lambda x: x.rolling(window=4, min_periods=1).sum())\n",
    "\n",
    "# Flag items for rebalancing based on low demand threshold\n",
    "test_data['rebalance_flag'] = np.where(\n",
    "    (test_data['is_peak_season'] & (test_data['low_demand_4weeks'] <= low_demand_threshold)) |\n",
    "    (~test_data['is_peak_season'] & (test_data['low_demand_4weeks'] <= (low_demand_threshold * 1.5))),\n",
    "    True,\n",
    "    False\n",
    ")\n",
    "\n",
    "\n",
    "# Summarize flagged items for rebalancing\n",
    "rebalance_needs = test_data[test_data['rebalance_flag']].groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'].sum().reset_index()\n",
    "rebalance_needs['rebalance_units'] = np.round(rebalance_needs['predicted_inventory_needs']).astype(int)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPredicted Inventory Needs (Test Period):\")\n",
    "print(inventory_needs)\n",
    "\n",
    "print(\"\\nPotential Rebalance Needs (Low Demand Thresholds):\")\n",
    "print(rebalance_needs)\n",
    "\n",
    "print(\"Total Predicted Inventory Needs (Test Period):\", int(inventory_needs['predicted_inventory_needs'].sum()))\n",
    "print(\"Total Rebalance Needs (Low Demand Adjustments):\", int(rebalance_needs['rebalance_units'].sum()))\n",
    "print(\"Total Actual Units Sold (Test Period):\", int(y_test.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Validation MAE: 9.378395205852595\n",
      "Extended Validation RMSE: 27.948262519348432\n",
      "Test MAE: 10.016\n",
      "Test RMSE: 32.26431226841579\n",
      "\n",
      "Predicted Inventory Needs (Test Period):\n",
      "      product_type_no  colour_group_code  predicted_inventory_needs\n",
      "0                  -1                  6                         69\n",
      "1                  -1                  7                         47\n",
      "2                  -1                  9                        433\n",
      "3                  -1                 10                        129\n",
      "4                  -1                 12                         58\n",
      "...               ...                ...                        ...\n",
      "1620              515                 11                          3\n",
      "1621              525                  3                          6\n",
      "1622              529                 31                         19\n",
      "1623              529                 52                         23\n",
      "1624              532                  9                         46\n",
      "\n",
      "[1625 rows x 3 columns]\n",
      "\n",
      "Potential Rebalance Needs (Low Demand Thresholds):\n",
      "     product_type_no  colour_group_code  predicted_inventory_needs  \\\n",
      "0                 49                 10                          2   \n",
      "1                 49                 51                          4   \n",
      "2                 57                 23                          2   \n",
      "3                 57                 30                          1   \n",
      "4                 57                 31                          3   \n",
      "..               ...                ...                        ...   \n",
      "545              504                 10                          1   \n",
      "546              508                 10                          5   \n",
      "547              512                  7                          3   \n",
      "548              515                 11                          3   \n",
      "549              525                  3                          6   \n",
      "\n",
      "     rebalance_units  \n",
      "0                  2  \n",
      "1                  4  \n",
      "2                  2  \n",
      "3                  1  \n",
      "4                  3  \n",
      "..               ...  \n",
      "545                1  \n",
      "546                5  \n",
      "547                3  \n",
      "548                3  \n",
      "549                6  \n",
      "\n",
      "[550 rows x 4 columns]\n",
      "Total Predicted Inventory Needs (Test Period): 226469\n",
      "Total Rebalance Needs (Low Demand Adjustments): 1805\n",
      "Total Actual Units Sold (Test Period): 234985\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Ensure 'week' is in datetime format without timezone\n",
    "data['week'] = pd.to_datetime(data['week']).dt.tz_localize(None)\n",
    "\n",
    "# Filter data up to March 2020 and sort\n",
    "data = data[data['week'] <= \"2020-03-01\"].copy()\n",
    "data = data.sort_values(by=['product_type_no', 'colour_group_code', 'week'])\n",
    "\n",
    "# Feature Engineering: Lagged Features and Seasonal Features\n",
    "data['lag_units_sold_1week'] = data.groupby(['product_type_no', 'colour_group_code'])['total_units_sold'].shift(1)\n",
    "data['lag_units_sold_2weeks'] = data.groupby(['product_type_no', 'colour_group_code'])['total_units_sold'].shift(2)\n",
    "data = data.dropna(subset=['lag_units_sold_1week', 'lag_units_sold_2weeks']).reset_index(drop=True)\n",
    "\n",
    "# Month feature for seasonality\n",
    "data['month'] = data['week'].dt.month\n",
    "\n",
    "# Scale the price\n",
    "data['average_price'] = data['average_price'] * 10\n",
    "\n",
    "# Define train, validation, and test sets\n",
    "train_data = data[data['week'] < \"2020-01-01\"]\n",
    "validation_data = data[(data['week'] >= \"2020-01-01\") & (data['week'] < \"2020-03-01\")]\n",
    "test_data = data[data['week'] == data['week'].max()]\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\n",
    "    'product_type_no', 'colour_group_code', 'average_price',\n",
    "    'lag_units_sold_1week', 'lag_units_sold_2weeks', 'month'\n",
    "]\n",
    "\n",
    "X_train, y_train = train_data[feature_columns], train_data['total_units_sold']\n",
    "X_validation, y_validation = validation_data[feature_columns], validation_data['total_units_sold']\n",
    "X_test, y_test = test_data[feature_columns], test_data['total_units_sold']\n",
    "\n",
    "# Initialize and train base models\n",
    "model_rf = RandomForestRegressor(n_estimators=200, max_depth=30, random_state=42)\n",
    "model_lr = LinearRegression()\n",
    "model_mlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=500, random_state=42)\n",
    "\n",
    "model_rf.fit(X_train, y_train)\n",
    "model_lr.fit(X_train, y_train)\n",
    "model_mlp.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions and round to integers\n",
    "preds_rf_val = np.round(model_rf.predict(X_validation)).astype(int)\n",
    "preds_lr_val = np.round(model_lr.predict(X_validation)).astype(int)\n",
    "preds_mlp_val = np.round(model_mlp.predict(X_validation)).astype(int)\n",
    "\n",
    "preds_rf_test = np.round(model_rf.predict(X_test)).astype(int)\n",
    "preds_lr_test = np.round(model_lr.predict(X_test)).astype(int)\n",
    "preds_mlp_test = np.round(model_mlp.predict(X_test)).astype(int)\n",
    "\n",
    "# Meta-model predictions\n",
    "val_predictions_df = pd.DataFrame({'RandomForest': preds_rf_val, 'LinearRegression': preds_lr_val, 'MLPRegressor': preds_mlp_val})\n",
    "test_predictions_df = pd.DataFrame({'RandomForest': preds_rf_test, 'LinearRegression': preds_lr_test, 'MLPRegressor': preds_mlp_test})\n",
    "\n",
    "final_model_rf = RandomForestRegressor(n_estimators=200, max_depth=30, random_state=42)\n",
    "final_model_rf.fit(val_predictions_df, y_validation)\n",
    "\n",
    "# Generate and round meta-model predictions\n",
    "final_preds_val = np.round(final_model_rf.predict(val_predictions_df)).astype(int)\n",
    "final_preds_test = np.round(final_model_rf.predict(test_predictions_df)).astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_val = mean_absolute_error(y_validation, final_preds_val)\n",
    "rmse_val = mean_squared_error(y_validation, final_preds_val, squared=False)\n",
    "mae_test = mean_absolute_error(y_test, final_preds_test)\n",
    "rmse_test = mean_squared_error(y_test, final_preds_test, squared=False)\n",
    "\n",
    "print(\"Extended Validation MAE:\", mae_val)\n",
    "print(\"Extended Validation RMSE:\", rmse_val)\n",
    "print(\"Test MAE:\", mae_test)\n",
    "print(\"Test RMSE:\", rmse_test)\n",
    "\n",
    "# Aggregate inventory needs by product type and color for the test period\n",
    "test_data['predicted_inventory_needs'] = final_preds_test\n",
    "inventory_needs = test_data.groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'].sum().reset_index()\n",
    "\n",
    "# Data-Driven Peak Season Identification Using Quantile-Based Thresholds\n",
    "\n",
    "# Calculate 90th percentile threshold for peak season within each product-type and color group\n",
    "demand_clusters = data.groupby(['product_type_no', 'colour_group_code', 'week'])['total_units_sold'].sum().reset_index()\n",
    "demand_clusters['peak_threshold'] = demand_clusters.groupby(['product_type_no', 'colour_group_code'])['total_units_sold'].transform(lambda x: x.quantile(0.90))\n",
    "\n",
    "# Flag as peak season if total_units_sold exceeds the 90th percentile threshold\n",
    "demand_clusters['is_peak_season'] = demand_clusters['total_units_sold'] >= demand_clusters['peak_threshold']\n",
    "\n",
    "# Merge peak season flags back into main dataset\n",
    "data = pd.merge(data, demand_clusters[['product_type_no', 'colour_group_code', 'week', 'is_peak_season']], \n",
    "                on=['product_type_no', 'colour_group_code', 'week'], how='left')\n",
    "\n",
    "# Insert the duplicate-cleaning code here\n",
    "if 'is_peak_season_x' in data.columns and 'is_peak_season_y' in data.columns:\n",
    "    data = data.drop(columns=['is_peak_season_x']).rename(columns={'is_peak_season_y': 'is_peak_season'})\n",
    "elif 'is_peak_season_x' in data.columns:\n",
    "    data = data.rename(columns={'is_peak_season_x': 'is_peak_season'})\n",
    "elif 'is_peak_season_y' in data.columns:\n",
    "    data = data.rename(columns={'is_peak_season_y': 'is_peak_season'})\n",
    "\n",
    "# Check and remove any remaining duplicates\n",
    "if data.columns.duplicated().any():\n",
    "    data = data.loc[:, ~data.columns.duplicated()]\n",
    "\n",
    "# Merge peak season flags back into main dataset\n",
    "data = pd.merge(data, demand_clusters[['product_type_no', 'colour_group_code', 'week', 'is_peak_season']], \n",
    "                on=['product_type_no', 'colour_group_code', 'week'], how='left')\n",
    "\n",
    "# Rebalance Strategy Using Low-Demand Thresholds Instead of Negative Values\n",
    "low_demand_threshold = 5  # Adjust based on data analysis\n",
    "\n",
    "# Calculate a rolling 4-week sum and apply low-demand threshold for rebalancing flag\n",
    "test_data['low_demand_4weeks'] = test_data.groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'] \\\n",
    "    .transform(lambda x: x.rolling(window=4, min_periods=1).sum())\n",
    "\n",
    "# Flag items for rebalancing based on low demand threshold\n",
    "test_data['rebalance_flag'] = np.where(\n",
    "    (test_data['is_peak_season'] & (test_data['low_demand_4weeks'] <= low_demand_threshold)) |\n",
    "    (~test_data['is_peak_season'] & (test_data['low_demand_4weeks'] <= (low_demand_threshold * 1.5))),\n",
    "    True,\n",
    "    False\n",
    ")\n",
    "\n",
    "# Summarize flagged items for rebalancing\n",
    "rebalance_needs = test_data[test_data['rebalance_flag']].groupby(['product_type_no', 'colour_group_code'])['predicted_inventory_needs'].sum().reset_index()\n",
    "rebalance_needs['rebalance_units'] = np.round(rebalance_needs['predicted_inventory_needs']).astype(int)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nPredicted Inventory Needs (Test Period):\")\n",
    "print(inventory_needs)\n",
    "\n",
    "print(\"\\nPotential Rebalance Needs (Low Demand Thresholds):\")\n",
    "print(rebalance_needs)\n",
    "\n",
    "print(\"Total Predicted Inventory Needs (Test Period):\", int(inventory_needs['predicted_inventory_needs'].sum()))\n",
    "print(\"Total Rebalance Needs (Low Demand Adjustments):\", int(rebalance_needs['rebalance_units'].sum()))\n",
    "print(\"Total Actual Units Sold (Test Period):\", int(y_test.sum()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Future Predictions with Rebalance Flags for 4 Weeks Out:\n",
      "      product_type_no product_type_name  colour_group_code colour_group_name  \\\n",
      "0                  -1           Unknown                  6        Light Grey   \n",
      "1                  -1           Unknown                  7              Grey   \n",
      "2                  -1           Unknown                  9             Black   \n",
      "3                  -1           Unknown                 10             White   \n",
      "4                  -1           Unknown                 12       Light Beige   \n",
      "...               ...               ...                ...               ...   \n",
      "6495              515         Straw hat                 11         Off White   \n",
      "6496              525          Keychain                  3            Silver   \n",
      "6497              529       Mobile case                 31      Light Orange   \n",
      "6498              529       Mobile case                 52              Pink   \n",
      "6499              532          Umbrella                  9             Black   \n",
      "\n",
      "      predicted_units_sold  week_ahead  rebalance_flag  \n",
      "0                       70           1           False  \n",
      "1                       47           1           False  \n",
      "2                      431           1           False  \n",
      "3                      133           1            True  \n",
      "4                       58           1           False  \n",
      "...                    ...         ...             ...  \n",
      "6495                     4           4           False  \n",
      "6496                    23           4            True  \n",
      "6497                    17           4           False  \n",
      "6498                    20           4            True  \n",
      "6499                    31           4           False  \n",
      "\n",
      "[6500 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame to store predictions for each future week\n",
    "future_predictions = pd.DataFrame()\n",
    "\n",
    "# Copy the last week's data to use for initial predictions\n",
    "last_week_data = test_data.copy()  # Assuming test_data is the final week of actual data\n",
    "\n",
    "# Initialize a rolling window for tracking low-demand predictions\n",
    "last_week_data['low_demand_4weeks'] = last_week_data['total_units_sold']  # Start with current units sold\n",
    "\n",
    "# Predict for 4 future weeks\n",
    "for week_ahead in range(1, 5):\n",
    "    # Prepare features for prediction using the last week's data\n",
    "    X_future = last_week_data[feature_columns]  # feature_columns should include all necessary features\n",
    "    \n",
    "    # Step 1: Generate predictions from each base model for the current week\n",
    "    preds_rf = model_rf.predict(X_future)\n",
    "    preds_lr = model_lr.predict(X_future)\n",
    "    preds_mlp = model_mlp.predict(X_future)\n",
    "    \n",
    "    # Step 2: Create DataFrame with base model predictions to input to the meta-model\n",
    "    base_preds_df = pd.DataFrame({\n",
    "        'RandomForest': preds_rf,\n",
    "        'LinearRegression': preds_lr,\n",
    "        'MLPRegressor': preds_mlp\n",
    "    })\n",
    "    \n",
    "    # Step 3: Use the meta-model to predict the final units sold\n",
    "    future_pred = final_model_rf.predict(base_preds_df)\n",
    "    \n",
    "    # Step 4: Store predictions in a DataFrame with the week number (1 to 4)\n",
    "    prediction_df = last_week_data[['product_type_no', 'product_type_name', \n",
    "                                    'colour_group_code', 'colour_group_name']].copy()\n",
    "    prediction_df['predicted_units_sold'] = np.round(future_pred).astype(int)  # Ensure integer values\n",
    "    prediction_df['week_ahead'] = week_ahead  # Label the future week\n",
    "    \n",
    "    # Step 5: Calculate rolling 4-week cumulative demand for the low-demand rebalancing feature\n",
    "    # Update `low_demand_4weeks` with the current prediction and check if it meets the threshold\n",
    "    last_week_data['low_demand_4weeks'] = last_week_data['low_demand_4weeks'] + future_pred - last_week_data['low_demand_4weeks'].shift(1).fillna(0)\n",
    "    \n",
    "    # Flag for rebalancing if cumulative demand falls below threshold\n",
    "    prediction_df['rebalance_flag'] = np.where(\n",
    "        (last_week_data['is_peak_season'] & (last_week_data['low_demand_4weeks'] <= low_demand_threshold)) |\n",
    "        (~last_week_data['is_peak_season'] & (last_week_data['low_demand_4weeks'] <= (low_demand_threshold * 1.5))),\n",
    "        True,\n",
    "        False\n",
    "    )\n",
    "    \n",
    "    # Append the predictions to the future_predictions DataFrame\n",
    "    future_predictions = pd.concat([future_predictions, prediction_df], ignore_index=True)\n",
    "    \n",
    "    # Step 6: Update last_week_data to simulate moving into the next week\n",
    "    # Shift lagged features: set 2-week lag to previous 1-week lag, and set 1-week lag to current prediction\n",
    "    last_week_data['lag_units_sold_2weeks'] = last_week_data['lag_units_sold_1week']\n",
    "    last_week_data['lag_units_sold_1week'] = future_pred  # Set new lag as current prediction\n",
    "    \n",
    "    # Update `total_units_sold` for accurate rolling calculations in the next week\n",
    "    last_week_data['total_units_sold'] = np.round(future_pred).astype(int)\n",
    "\n",
    "# Display the resulting DataFrame with predictions and rebalancing flags for 4 future weeks\n",
    "print(\"\\nFuture Predictions with Rebalance Flags for 4 Weeks Out:\")\n",
    "print(future_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table successfully pushed to the database!\n"
     ]
    }
   ],
   "source": [
    "# Set up the schema if needed and push inventory_needs to PostgreSQL\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(f'SET search_path TO {my_schema};'))\n",
    "    future_predictions.to_sql('future_predictions', con=engine, schema=my_schema, if_exists='replace', index=False)\n",
    "\n",
    "print(\"Table successfully pushed to the database!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
